{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder for anomaly detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds \n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    " \n",
    "import os \n",
    "import random \n",
    "from IPython import display "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting parameters \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for building the model and training\n",
    "BATCH_SIZE = \n",
    "LATENT_DIM = \n",
    "IMAGE_SIZE = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_slices_paths(image_dir):\n",
    "    ''' returns a list of paths to the image files'''\n",
    "    image_file_list = os.listdir(image_dir)\n",
    "    image_paths = [os.path.join(image_dir, fname) for fname in image_file_list]\n",
    "\n",
    "    return image_paths \n",
    "\n",
    "def map_image(image_filename): \n",
    "    ''' preprocess the images'''\n",
    "    img_raw = tf.io.read_file(image_filename)\n",
    "    image = tf.image.decode_jpeg(img_raw) # depends on the type of images\n",
    "\n",
    "    image = tf.cast(image, dtype=tf.float32)\n",
    "    image = tf.image.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "    image = image/255.0\n",
    "    image = tf.reshape(image, shape=(IMAGE_SIZE, IMAGE_SIZE,3))\n",
    "\n",
    "    return image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list containing the image paths\n",
    "paths = get_dataset_slices_paths(image_dir = )\n",
    "\n",
    "# shuffle the paths \n",
    "random.shuffle(paths)\n",
    "\n",
    "# split the paths list into training and validation sets \n",
    "paths_len = len(paths)\n",
    "train_paths_len = int(paths_len*0.8)\n",
    "\n",
    "train_paths = paths[:train_paths_len]\n",
    "val_paths = paths[train_paths_len:]\n",
    "\n",
    "# load the training image paths into tensors, creates batches and shuffle\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices((train_paths))\n",
    "training_dataset = training_dataset.map(map_image)\n",
    "training_dataset = training_dataset.shuffle().batch(BATCH_SIZE)\n",
    "\n",
    "# load the validation image paths into tensors, creates batches \n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((val_paths))\n",
    "validation_dataset = validation_dataset.map(map_image)\n",
    "validation_dataset = validation_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "print(f'number of batches in the training set: {len(training_dataset)}')\n",
    "print(f'number of batches in the validation set: {len(validation_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_fabric(dataset, size):\n",
    "    ''' Takes a sample from a dataset batch and plots it in a grid. '''\n",
    "    dataset = dataset.unbatch().take(size)\n",
    "    n_cols = 2\n",
    "    n_rows = size//n_cols+1\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    i=0\n",
    "    for image in dataset:\n",
    "        i+=1 \n",
    "        disp_image = np.reshape(image, (64,64,3))\n",
    "        plt.subplot(n_rows, n_cols, i)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.imshow(disp_image)\n",
    "\n",
    "def display_one_row(dis_images, offset, shape=(28,28)):\n",
    "    ''' Display a row of images. '''\n",
    "    for i, img in enumerate(dis_images):\n",
    "        plt.subplot(2,2,offset+i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        image = np.reshape(img, shape)\n",
    "        plt.imshow(img)\n",
    "\n",
    "def display_results(dis_input_images, dis_predicted):\n",
    "    ''' Display input and predicted images. '''\n",
    "    plt.figure(figsize=(15,5))\n",
    "    display_one_row(dis_input_images, 0, shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    display_one_row(dis_predicted, 20, shape=(IMAGE_SIZE, IMAGE_SIZE, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_fabric(validation_dataset, size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This layer provide the Gaussian noise input along with the mean ($\\mu $) and standard deviation ($\\sigma$) of the encoder's output, according to the following equation:\n",
    "**$$z = \\mu + e^{0.5\\sigma} * \\epsilon $$**\n",
    "($\\epsilon$ = random sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        ''' Generates a random sample and combines with the encoder output\n",
    "        Args:\n",
    "        inputs - output tensor from the encoder \n",
    "        Returns: \n",
    "        'inputs' tensors combined with a random sample\n",
    "        '''\n",
    "        mu, sigma = inputs\n",
    "        batch = tf.shape(mu)[0]\n",
    "        dim = tf.shape(mu)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        z = mu + np.exp^(0.5*sigma)*epsilon\n",
    "        return z \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_block(tf.keras.Model):   \n",
    "    def __init__(self, inputs, final_conv, repetition):\n",
    "        super(Encoder_block, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(32, 3, 2, 'same')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, 3, 2, 'same')\n",
    "        self.conv3 = tf.keras.layers.Conv2D(128, 3, 2, 'same')\n",
    "        self.lrelu = tf.keras.layers.LeakyReLU(0.1)\n",
    "        self.bn = tf.keras.layers.BatchNormalization()\n",
    "        self.inputs = inputs\n",
    "        self.repetition = repetition\n",
    "        self.final_conv= final_conv\n",
    "    def call(self,final_conv=False):\n",
    "        x = self.conv1(self.inputs)\n",
    "        x = self.lrelu(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.lrelu(x)\n",
    "        x = self.bn(x)\n",
    "        feature_shape = self.conv3(x)\n",
    "        \n",
    "\n",
    "        if final_conv: \n",
    "            return x\n",
    "        else:\n",
    "            return x, x.shape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, latent_dim, input_shape):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = tf.keras.layers.Input(shape=self.input_shape)\n",
    "        x = Encoder_block(inputs, filters=32, final_conv=False)\n",
    "        x = Encoder_block(x, filters=64, final_conv=False)\n",
    "        x = Encoder_block(x, filters=128, final_conv=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
